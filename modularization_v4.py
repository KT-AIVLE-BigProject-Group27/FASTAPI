################################################################################################
# í•„ìš” íŒ¨í‚¤ì§€ import
################################################################################################
import pickle, openai, torch, json, os, re, fitz, numpy as np, torch.nn as nn
from sklearn.metrics.pairwise import cosine_similarity
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, ElectraTokenizer, ElectraModel, BertTokenizer, BertModel, AutoModelForCausalLM, AutoTokenizer
from kobert_transformers import get_tokenizer

################################################################################################
# PDFíŒŒì¼ì—ì„œ Text ì¶”ì¶œ
################################################################################################
def extract_text_from_pdf(pdf_path):
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}")
    doc = fitz.open(pdf_path)
    text = ""
    for page_num in range(doc.page_count):
        page = doc.load_page(page_num)
        text += page.get_text()
    text = text.replace('\n', ' ')
    text = re.sub(r'-\s?\d+\s?-', '', text)
    return text
################################################################################################
# # ë‚ ì§œ í˜•ì‹ ë³€í˜• (ì˜ˆ: 2023. 01. 01. => 2023-01-01)
################################################################################################
def replace_date_with_placeholder(content):
    content_with_placeholder = re.sub(r"(\d{4})\.\s(\d{2})\.\s(\d{2}).", r"\1-\2-\3", content)
    return content_with_placeholder
################################################################################################
# ì „ì²´ ê³„ì•½ì„œ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ì¡°ë¥¼ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ ver2
################################################################################################
def contract_to_articles(txt):
    pattern, key_pattern = r'(ì œ\d+ì¡°(?:ì˜\d+)? \[.+?\])', r'ì œ(\d+)ì¡°(?:ì˜(\d+))? \[.+?\]'
    matches = list(re.finditer(pattern, txt))
    contract_sections = {}
    for i, match in enumerate(matches):
        section_title = match.group(0)
        start_idx = match.end()
        end_idx = matches[i + 1].start() if i + 1 < len(matches) else len(txt)
        section_content = txt[start_idx:end_idx].strip()
        section_content = section_content.replace('â€œ', '').replace('â€', '').replace('\n', '').replace('<í‘œ>','')
        match_title = re.match(key_pattern, section_title)
        main_num = match_title.group(1)
        try:
            sub_num = match_title.group(2)
        except IndexError:
            sub_num = None
        key = f"{main_num}-{sub_num}" if sub_num else main_num
        contract_sections[key] = section_title + ' ' + section_content
    return contract_sections
################################################################################################
# ì¡°ë¥¼ ë°›ì•„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
################################################################################################
def split_once_by_clauses(content):
    pattern = r"(â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©)"
    matches = list(re.finditer(pattern, content))
    result = []
    for i, match in enumerate(matches):
        end = match.end()
        if i + 1 < len(matches):
            next_start = matches[i + 1].start()
            clause_content = content[end:next_start].strip()
        else:
            clause_content = content[end:].strip()
        result.append(match.group())
        result.append(clause_content)
    return result

def split_once_by_sub_clauses(content):
    pattern = r"(\d+\.)"
    matches = list(re.finditer(pattern, content))
    result = []
    for i, match in enumerate(matches):
        end = match.end()
        if i + 1 < len(matches):
            next_start = matches[i + 1].start()
            clause_content = content[end:next_start].strip()
        else:
            clause_content = content[end:].strip()
        result.append(match.group())
        result.append(clause_content)
    return result


def article_to_sentences(article_number,article_title, article_content):
    sentences = []
    if 'â‘ ' in article_content:
        clause_sections = split_once_by_clauses(article_content)
        for i in range(0, len(clause_sections), 2):
            clause_number = clause_sections[i]
            clause_content = clause_sections[i + 1]
            if '1.' in clause_content:
                sub_clause_sections = split_once_by_sub_clauses(clause_content)
                sentences.append([article_number.strip(), article_title.strip(), '', clause_number.strip(),clause_content.split('1.')[0].strip(), '', ''])
                for j in range(0, len(sub_clause_sections), 2):
                    sub_clause_number = sub_clause_sections[j]
                    sub_clause_content = sub_clause_sections[j + 1]
                    sentences.append([article_number.strip(), article_title.strip(), '', clause_number.strip(), clause_content.split('1.')[0].strip(), sub_clause_number[0].strip(), sub_clause_content.strip()])
            else:
                sentences.append([article_number.strip(), article_title.strip(), '', clause_number.strip(), clause_content.strip(), '', ''])
    elif '1.' in article_content:
        sub_clause_sections = split_once_by_sub_clauses(article_content)
        sentences.append([article_number.strip(), article_title.strip(), article_content.split('1.')[0].strip(), '', '', '', ''])
        for j in range(0, len(sub_clause_sections), 2):
            sub_clause_number = sub_clause_sections[j]
            sub_clause_content = sub_clause_sections[j + 1]
            sentences.append([article_number.strip(), article_title.strip(), article_content.split('1.')[0].strip(), '', '', sub_clause_number.strip(),sub_clause_content.strip()])
    else:
        sentences.append([article_number.strip(),article_title.strip(),article_content.strip(),'','','',''])
    return sentences
################################################################################################
# ëª¨ë¸ ë¡œë“œ
################################################################################################
def load_trained_model_statice(model_class, model_file):
    model = model_class().to(device)
    state_dict = torch.load(model_file, map_location=device)
    if isinstance(state_dict, dict):
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        return model
    else:
        raise TypeError(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {model_file} (ì˜ëª»ëœ ë°ì´í„° íƒ€ì… {type(state_dict)})")

################################################################################################
# í† í¬ë‚˜ì´ì§• ëª¨ë¸ ë¡œë“œ & ì „ì²´ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ
################################################################################################
def initialize_models():
    global clause_mapping, unfair_model, unfair_tokenizer, article_model, article_tokenizer, toxic_model, toxic_tokenizer,law_tokenizer,law_model, law_data, law_embeddings, device, summary_model, summary_tokenizer

    clause_mapping = {"â‘ ": '1',"â‘¡": '2',"â‘¢": '3',"â‘£": '4',"â‘¤": '5',"â‘¥": '6',"â‘¦": '7',"â‘§": '8',"â‘¨": '9',"â‘©": '10'}

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print('law model loading...')
    law_tokenizer = BertTokenizer.from_pretrained("klue/bert-base")
    law_model = BertModel.from_pretrained("klue/bert-base").to(device)
    print('summary model loading...')
    summary_model = AutoModelForCausalLM.from_pretrained('./Model/article_summary',trust_remote_code=True)
    summary_tokenizer = AutoTokenizer.from_pretrained('./Model/article_summary',trust_remote_code=True)
    class Unfair_KoBERTMLPClassifier(nn.Module):
        def __init__(self):
            super(Unfair_KoBERTMLPClassifier, self).__init__()
            self.bert = BertModel.from_pretrained("monologg/kobert")
            self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.3)
            self.fc2 = nn.Linear(256, 1)
            self.sigmoid = nn.Sigmoid()

        def forward(self, input_ids, attention_mask):
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            cls_output = outputs.last_hidden_state[:, 0, :]
            x = self.fc1(cls_output)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.fc2(x)
            return self.sigmoid(x)

    class Toxic_KoELECTRAMLPClassifier(nn.Module):
        def __init__(self):
            super(Toxic_KoELECTRAMLPClassifier, self).__init__()
            self.electra = ElectraModel.from_pretrained("monologg/koelectra-base-discriminator")
            self.fc1 = nn.Linear(self.electra.config.hidden_size, 256)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.3)
            self.fc2 = nn.Linear(256, 1)
            self.sigmoid = nn.Sigmoid()

        def forward(self, input_ids, attention_mask):
            outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)
            cls_output = outputs.last_hidden_state[:, 0, :]  # ì²« í† í° í™œìš©
            x = self.fc1(cls_output)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.fc2(x)
            return self.sigmoid(x)

    class Article_KoBERTMLPClassifier(nn.Module):
        def __init__(self):
            super(Article_KoBERTMLPClassifier, self).__init__()
            self.bert = BertModel.from_pretrained("monologg/kobert")
            self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.3)
            self.fc2 = nn.Linear(256, 8)

        def forward(self, input_ids, attention_mask):
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            cls_output = outputs.last_hidden_state[:, 0, :]
            x = self.fc1(cls_output)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.fc2(x)
            return x

    # ë¶ˆê³µì • ì¡°í•­ íŒë³„ ëª¨ë¸ ë¡œë“œ
    print('unfair model loading...')
    unfair_model = load_trained_model_statice(Unfair_KoBERTMLPClassifier, f"./Model/unfair_identification/KoBERT_mlp.pth")
    unfair_tokenizer = get_tokenizer()
    # ì¡°í•­ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
    print('article model loading...')
    article_model = load_trained_model_statice(Article_KoBERTMLPClassifier, f"./Model//article_prediction/KoBERT_mlp.pth")
    article_tokenizer = BertTokenizer.from_pretrained("monologg/kobert")
    # ë…ì†Œ ì¡°í•­ íŒë³„ ëª¨ë¸ ë¡œë“œ
    print('toxic model loading...')
    toxic_model = load_trained_model_statice(Toxic_KoELECTRAMLPClassifier, f"./Model/toxic_identification/KoELECTRA_mlp.pth")
    toxic_tokenizer =ElectraTokenizer.from_pretrained("monologg/koelectra-base-discriminator")
    # ë²•ë¥  ë°ì´í„° ë¡œë“œ
    with open("./Data_Analysis/law/law_embeddings.pkl", "rb") as f:
        data = pickle.load(f)
    with open("./Data_Analysis/law/law_data_ver2.json", "r", encoding="utf-8") as f:
        law_data = json.load(f)
    law_embeddings = np.array(data["law_embeddings"])
    print("All models and data have been successfully loaded")
################################################################################################
# ë¶ˆê³µì • ì‹ë³„
################################################################################################
def predict_unfair_clause(sentence):
    unfair_model.eval()
    inputs = unfair_tokenizer(sentence, padding=True, truncation=True, max_length=256, return_tensors="pt").to(device)
    with torch.no_grad():
        output = unfair_model(inputs["input_ids"], inputs["attention_mask"])
        unfair_prob = output.item()
    return 1 if unfair_prob >= 0.311002 else 0, unfair_prob
################################################################################################
# ì¡°í•­ ì˜ˆì¸¡
################################################################################################
def predict_article(sentence):
    idx_to_article = {0: '11', 1: '14', 2: '15', 3: '16', 4: '17', 5: '19', 6: '7', 7: '12'}
    article_model.eval()
    inputs = article_tokenizer(sentence, padding=True, truncation=True, max_length=256, return_tensors="pt").to(device)
    with torch.no_grad():
        output = article_model(inputs["input_ids"], inputs["attention_mask"])
        predicted_idx = idx_to_article[torch.argmax(output).item()]
    return predicted_idx
################################################################################################
# ë…ì†Œ ì‹ë³„
################################################################################################
def predict_toxic_clause(sentence):
    toxic_model.eval()
    inputs = toxic_tokenizer(sentence, padding=True, truncation=True, max_length=256, return_tensors="pt").to(device)
    with torch.no_grad():
        output = toxic_model(inputs["input_ids"], inputs["attention_mask"])
        toxic_prob = output.item()
    return 1 if toxic_prob >= 0.586741 else 0, toxic_prob
################################################################################################
# ì½”ì‚¬ì¸ ìœ ì‚¬ë„
################################################################################################
def find_most_similar_law_within_article(sentence, predicted_article, law_data):
    contract_embedding = law_model(**law_tokenizer(sentence, return_tensors="pt", padding=True, truncation=True).to(device)).pooler_output.cpu().detach().numpy()[0]
    predicted_article = str(predicted_article)
    matching_article = []
    for article in law_data:
        if article["article_number"].split()[1].startswith(predicted_article):
            matching_article.append(article)
    if not matching_article:
        return {
            "Article number": None,
            "Article title": None,
            "clause number": None,
            "subclause number": None,
            "Article detail": None,
            "clause detail": None,
            "subclause detail": None,
            "similarity": None
        }
    best_match = None
    best_similarity = -1
    for article in matching_article:
        article_title = article.get("article_title", None)
        article_detail = article.get("article_content", None)
        if article_title:
            article_embedding = law_model(**law_tokenizer(article_title, return_tensors="pt", padding=True, truncation=True).to(device)).pooler_output.cpu().detach().numpy()[0]
            similarity = cosine_similarity([contract_embedding], [article_embedding])[0][0]
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = {
                    "article_number": article["article_number"],
                    "article_title": article_title,
                    "article_detail": article_detail,
                    "clause_number": None,
                    "clause_detail": None,
                    "subclauses": None,
                    "similarity": best_similarity
                }
        for clause in article.get("clauses", []):
            clause_text = clause["content"].strip()
            clause_number = clause["clause_number"]
            if clause_text:
                clause_embedding = law_model(**law_tokenizer(clause_text, return_tensors="pt", padding=True, truncation=True).to(device)).pooler_output.cpu().detach().numpy()[0]
                similarity = cosine_similarity([contract_embedding], [clause_embedding])[0][0]
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = {
                        "article_number": article["article_number"],
                        "article_title": article_title,
                        "article_detail": article_detail,
                        "clause_number": clause_number,
                        "clause_detail": clause_text,
                        "subclauses": clause.get("sub_clauses", []),
                        "similarity": best_similarity
                    }
        if best_match and best_match["subclauses"]:
            for subclause in best_match["subclauses"]:
                if isinstance(subclause, str):
                    subclause_embedding = law_model(**law_tokenizer(subclause, return_tensors="pt", padding=True, truncation=True).to(device)).pooler_output.cpu().detach().numpy()[0]
                    similarity = cosine_similarity([contract_embedding], [subclause_embedding])[0][0]
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_match["subclause_detail"] = subclause
    if best_match is None:
        return {
            "Article number": f"Article {predicted_article}",
            "Article title": None,
            "clause number": None,
            "subclause number": None,
            "Article detail": None,
            "clause detail": None,
            "subclause detail": None,
            "similarity": None
        }
    return {
        "Article number": best_match["article_number"],
        "Article title": best_match["article_title"],
        "clause number": f"{best_match['clause_number']}" if best_match["clause_number"] else None,
        "subclause number": best_match.get("subclause_detail")[0] if best_match.get("subclause_detail") else None,
        "Article detail": best_match["article_detail"],
        "clause detail": best_match["clause_detail"],
        "subclause detail": best_match.get("subclause_detail", None),
        "similarity": best_similarity
    }
################################################################################################
# ì„¤ëª… AI
################################################################################################
def explanation_AI(sentence, unfair_label, toxic_label, law=None):
    #with open('./Key/openAI_key.txt', 'r') as file:
    #    openai.api_key = file.readline().strip()
    os.environ['OPENAI_API_KEY'] = openai.api_key 
    client = openai.OpenAI()
    if unfair_label == 0 and toxic_label == 0:
        return None

    if unfair_label == 1:
        prompt = f"""
            ì•„ë˜ ê³„ì•½ ì¡°í•­ì´ íŠ¹ì • ë²•ë¥ ì„ ìœ„ë°˜í•˜ëŠ”ì§€ ë¶„ì„í•˜ê³ , í•´ë‹¹ ë²• ì¡°í•­(ì œnì¡° ì œmí•­ ì œzí˜¸)ì„ ìœ„ë°˜í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

            ê³„ì•½ ì¡°í•­: "{sentence}"
            ê´€ë ¨ ë²• ì¡°í•­: {law if law else "ê´€ë ¨ ë²• ì¡°í•­ ì—†ìŒ"}

            ì„¤ëª…ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
            "ì–´ë–¤ ë²•ì˜ nì¡° mí•­ zí˜¸ë¥¼ ìœ„ë°˜í–ˆìŠµë‹ˆë‹¤. ì´ìœ ~~~"

            âš ï¸ ì œê³µëœ ë²• ì¡°í•­ì´ ì‹¤ì œë¡œ ìœ„ë°˜ëœ ì¡°í•­ì´ ì•„ë‹ ê²½ìš°, GPTê°€ íŒë‹¨í•œ ì ì ˆí•œ ë²• ì¡°í•­ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
        """
    elif toxic_label == 1:
        prompt = f"""
            ì•„ë˜ ê³„ì•½ ì¡°í•­ì´ ë…ì†Œ ì¡°í•­ì¸ì§€ ë¶„ì„í•˜ê³ , ë…ì†Œ ì¡°í•­ì´ë¼ë©´ ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

            ê³„ì•½ ì¡°í•­: "{sentence}"

            ì„¤ëª…ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
            "ë¬´ì—‡ë¬´ì—‡ ë•Œë¬¸ì— ë…ì†Œì…ë‹ˆë‹¤."
        """

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system",
             "content": "ë‹¹ì‹ ì€ ê³„ì•½ì„œ ì¡°í•­ì´ íŠ¹ì • ë²•ë¥ ì„ ìœ„ë°˜í•˜ëŠ”ì§€ ë¶„ì„í•˜ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \në¶ˆê³µì • ì¡°í•­ì˜ ê²½ìš°, ì–´ë–¤ ë²• ì¡°í•­ì„ ìœ„ë°˜í–ˆëŠ”ì§€ ì¡°í•­(ì œnì¡° ì œmí•­ ì œzí˜¸) í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ëª…ì‹œí•œ í›„ ì„¤ëª…í•˜ì„¸ìš”. \nì œê³µëœ ë²• ì¡°í•­ì´ ì‹¤ì œë¡œ ìœ„ë°˜ëœ ì¡°í•­ì´ ì•„ë‹ ê²½ìš°, GPTê°€ íŒë‹¨í•œ ì ì ˆí•œ ë²• ì¡°í•­ì„ ì‚¬ìš©í•˜ì„¸ìš”. \në…ì†Œ ì¡°í•­ì€ ë²•ë¥  ìœ„ë°˜ì´ ì•„ë‹ˆë¼ ê³„ì•½ ë‹¹ì‚¬ìì—ê²Œ ë¯¸ì¹˜ëŠ” ìœ„í—˜ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n ë°˜ë“œì‹œ 200 token ì´í•˜ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
        max_tokens=500
    ).choices[0].message.content

    return response.strip()
################################################################################################
# ìš”ì•½ AI
################################################################################################
def article_summary_AI(article_content):
    prompt = f"""
        ì›ë³¸ ë¬¸ì¥:{article_content} \n
        ì›ë³¸ ë¬¸ì¥ì˜ ë§¥ë½ì„ ì‚´í´ë³´ê³ , ë¹ ë¥´ê²Œ ë¬¸ì¥ì„ ìš”ì•½í•˜ì—¬ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
        ì œëª©ì€ ê·¸ëŒ€ë¡œ ë‘ì‹œê³ , ë‚´ìš©ì˜ í•µì‹¬ì„ ì¶”ì¶œí•˜ì—¬ ì „ì²´ì ìœ¼ë¡œ ìš”ì•½í•˜ë©´ ë©ë‹ˆë‹¤.
        ê²°ê³¼ëŠ” í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë©ë‹ˆë‹¤.
        ë§ ëì„ ë²ˆì—­ë¬¸ì´ ì•„ë‹ˆë¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œê¸€ ë¬¸ì¥ì´ ë˜ë„ë¡ ê°€ê³µí•©ë‹ˆë‹¤.
        ê´„í˜¸ () ì† ë‚´ìš© ë³´ë‹¤ëŠ” ë¬¸ì¥ ì „ì²´ì˜ ë§¥ë½ì„ ë” ì¤‘ìš”í•˜ê²Œ ë´…ë‹ˆë‹¤.
        ë¬¸ì¥ì„ ìƒì„±í•  ë•Œ, 'ë‹¤' ë¡œ ëë‚˜ê²Œ í•©ë‹ˆë‹¤.
        ë¬¸ì¥ì˜ ì¡°ê¸ˆë§Œ ë” ê°„ëµí•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    messages = [
        {"role": "system",
         "content": "You are an excellent sentence summarizer. You understand the context and concisely summarize key sentences as an assistant."},
        {"role": "user", "content": prompt}
    ]
    summary_model.to(device)
    input_ids = summary_tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt",).to(device)
    output = summary_model.generate(input_ids, eos_token_id=summary_tokenizer.eos_token_id, max_new_tokens=512, do_sample=False)
    generated_text = summary_tokenizer.decode(output[0], skip_special_tokens=True)
    generated_text_only = generated_text[len(summary_tokenizer.decode(input_ids[0], skip_special_tokens=True)):]
    summary = generated_text_only.strip()
    summary = re.sub(r"\*\*ìš”ì•½ ë¬¸ì¥:\*\*:\s*", "", summary)
    summary = re.sub(r"\*\*ìš”ì•½ ë¬¸ì¥:\*\*:", "", summary)
    summary = re.sub(r"\*\*ìš”ì•½ ë¬¸ì¥:\*\*", "", summary)
    summary = re.sub("\n", "", summary)
    summary = re.sub(r"\*\*ìš”ì•½:\*\*", "", summary)
    summary = re.sub(r"\(.*?\)", "", summary)
    return summary
################################################################################################
# íŒŒì´í”„ ë¼ì¸
################################################################################################

async def pipline(contract_path,websocket_connections):
    indentification_results = []
    summary_results = []
 
    # 1ï¸âƒ£ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    yield {"step": "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."}
    txt = extract_text_from_pdf(contract_path)

    # 3ï¸âƒ£ ê³„ì•½ì„œë¥¼ ì¡° ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    yield {"step": "ê³„ì•½ì„œë¥¼ ì¡°í•­ë³„ë¡œ ë¶„ë¦¬ ì¤‘..."}
    print('Splitting text into article sections...')
    txt = replace_date_with_placeholder(txt)
    articles = contract_to_articles(txt)


    # 4ï¸âƒ£ ê³„ì•½ì„œ ìš”ì•½ì¤‘ 
    for article_number, article_detail in articles.items():
        print(f'Analyzing Article {article_number}...')
        yield {"step": "ê³„ì•½ì„œë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘..."}
        match = re.match(r"(ì œ\s?\d+ì¡°(?:ì˜\s?\d+)?\s?)\[(.*?)\]\s?(.+)", article_detail, re.DOTALL)
        article_title = match.group(2)
        article_content = match.group(3)
        sentences = article_to_sentences(article_number,article_title, article_content)
        summary = article_summary_AI(article_content)
        summary_results.append(
                        {
                        'article_number':article_number, # ì¡° ë²ˆí˜¸
                        'article_title': article_title, # ì¡° ì œëª©
                        'summary':  f"ì œ{article_number.split('-')[0]}ì¡°ì˜{article_number.split('-')[1]} [{article_title}] {summary}" if '-' in article_number else f"ì œ{article_number}ì¡° [{article_title}] {summary}"
                        }
        )

        # 4ï¸âƒ£ ë…ì†Œ ì¡°í•­ & ë¶ˆê³µì • ì¡°í•­ ì‹ë³„
        for article_number, article_title, article_content, clause_number, clause_detail, subclause_number, subclause_detail in sentences:
            sentence = re.sub(r'\s+', ' ', f'[{article_title}] {article_content} {clause_number} {clause_detail} {subclause_number + "." if subclause_number else ""} {subclause_detail}').strip()
            unfair_result, unfair_percent = predict_unfair_clause(sentence)
            
            if unfair_result:
                yield {"step": "ìœ„ë²• ì¡°í•­ì„ ì‹ë³„í•˜ëŠ” ì¤‘..."}                
                predicted_article = predict_article(sentence)  # ì˜ˆì¸¡ëœ ì¡°í•­
                law_details = find_most_similar_law_within_article(sentence, predicted_article, law_data)
                toxic_result = 0
                toxic_percent = 0
            else:
                yield {"step": "ë…ì†Œì¡°í•­ì„ ì‹ë³„í•˜ëŠ” ì¤‘..."}                
                toxic_result, toxic_percent = predict_toxic_clause(sentence)
                law_details = {
                    "Article number": None,
                    "Article title": None,
                    "clause number": None,
                    "subclause number": None,
                    "Article detail": None,
                    "clause detail": None,
                    "subclause detail": None,
                    "similarity": None
                }
            
            law_text = []
            if law_details.get("Article number"):
                law_text.append(f"{law_details['Article number']}({law_details['Article title']})")
            if law_details.get("Article detail"):
                law_text.append(f": {law_details['Article detail']}")
            if law_details.get("clause number"):
                law_text.append(f" {law_details['clause number']}: {law_details['clause detail']}")
            if law_details.get("subclause number"):
                law_text.append(f" {law_details['subclause number']}: {law_details['subclause detail']}")
            law = "".join(law_text) if law_text else None

            explain = explanation_AI(sentence, unfair_result, toxic_result, law)

            if unfair_result or toxic_result:

                indentification_results.append(
                                {
                                    'contract_article_number': article_number if article_number != "" else None, # ê³„ì•½ì„œ ì¡°
                                    'contract_clause_number' : clause_mapping[clause_number] if clause_number != "" else None, # ê³„ì•½ì„œ í•­
                                    'contract_subclause_number': subclause_number if subclause_number != "" else None, # ê³„ì•½ì„œ í˜¸
                                    'Sentence': sentence, # ì‹ë³„
                                    'Unfair': unfair_result, # ë¶ˆê³µì • ì—¬ë¶€
                                    'Unfair_percent': unfair_percent, # ë¶ˆê³µì • í™•ë¥ 
                                    'Toxic': toxic_result,  # ë…ì†Œ ì—¬ë¶€
                                    'Toxic_percent': toxic_percent,  # ë…ì†Œ í™•ë¥ 
                                    'law_article_number': law_details['Article number'],  # ì–´ê¸´ ë²• ì¡°   (ë¶ˆê³µì • 1ì¼ë•Œ, ì•„ë‹ˆë©´ None)
                                    'law_clause_number_law': law_details['clause number'], # ì–´ê¸´ ë²• í•­ (ë¶ˆê³µì • 1ì¼ë•Œ, ì•„ë‹ˆë©´ None)
                                    'law_subclause_number_law': law_details['subclause number'],  # ì–´ê¸´ ë²• í˜¸ (ë¶ˆê³µì • 1ì¼ë•Œ, ì•„ë‹ˆë©´ None)
                                    'explain': explain #explain (ë¶ˆê³µì • 1ë˜ëŠ” ë…ì†Œ 1ì¼ë•Œ, ì•„ë‹ˆë©´ None)
                                    }
                )

    yield {"step": "ë¶„ì„ì´ ì™„ë£ŒëìŠµë‹ˆë‹¤!"}
    yield ({"done": True, "indentification_results" : indentification_results ,"summary_results" : summary_results})

    #return indentification_results, summary_results
    ############################################################################################################
    ############################################################################################################

"""
import asyncio

async def pipline_with_progress(contract_path, websocket=None):
    indentification_results = []
    summary_results = []
    
    async def send_status_safe(message):
        #WebSocketì´ ì—´ë ¤ ìˆì„ ë•Œë§Œ ë©”ì‹œì§€ ì „ì†¡
        if websocket and websocket.client_state.name == "CONNECTED":
            try:
                await websocket.send_json({"step": message})
            except Exception as e:
                print(f"âš ï¸ WebSocket ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
                return False
        return True

    # ì›¹ì†Œì¼“ì„ í†µí•´ ì§„í–‰ ìƒíƒœ ì „ì†¡í•˜ëŠ” í•¨ìˆ˜
    async def send_status(message):
        if websocket:
            try:
                await websocket.send_json({"step": message})
            except Exception as e:
                print(f"âš ï¸ WebSocket ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")
                return False  # WebSocketì´ ë‹«íŒ ê²½ìš° False ë°˜í™˜
        return True  # ì •ìƒ ì „ì†¡ ì‹œ True ë°˜í™˜

    # 1ï¸âƒ£ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    print('ğŸš€ í•œê¸€ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...')
    yield {"step": "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."}
    txt = hwp5txt_to_string(hwp5txt_exe_path, contract_path)

    # 2ï¸âƒ£ AI ëª¨ë¸ ë¡œë“œ
    print('âš¡ AI ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...')
    yield {"step": "AI ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."}
    initialize_models()

    # 3ï¸âƒ£ ê³„ì•½ì„œë¥¼ ì¡° ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    print('ğŸ“Œ ê³„ì•½ì„œë¥¼ ì¡°í•­ë³„ë¡œ ë¶„ë¦¬ ì¤‘...')
    articles = contract_to_articles_ver2(txt)

    # 4ï¸âƒ£ ë…ì†Œ ì¡°í•­ & ë¶ˆê³µì • ì¡°í•­ ì‹ë³„
    print('ğŸ” ë…ì†Œ ë° ë¶ˆê³µì • ì¡°í•­ ì‹ë³„ ì¤‘...')
    yield {"step": "ë…ì†Œ ì¡°í•­ì„ ì‹ë³„í•˜ëŠ” ì¤‘..."}

    for article_number, article_detail in articles.items():
        match = re.match(r"(ì œ\s?\d+ì¡°(?:ì˜\s?\d+)?\s?)\[(.*?)\]\s?(.+)", article_detail, re.DOTALL)
        if not match:
            continue

        article_title = match.group(2)
        article_content = match.group(3)
        sentences = article_to_sentences(article_number, article_title, article_content)

        for _, _, _, clause_number, clause_detail, subclause_number, subclause_detail in sentences:
            for idx, sentence in enumerate([clause_detail, subclause_detail]):
                if not sentence.strip():
                    continue

                yield {"step": "ë¶ˆê³µì • ì¡°í•­ì„ ì‹ë³„í•˜ëŠ” ì¤‘..."}
                unfair_result, unfair_percent = predict_unfair_clause(unfair_model, sentence, 0.5011)

                if not unfair_result:
                    yield {"step": "ë…ì†Œ ì¡°í•­ì„ ì‹ë³„í•˜ëŠ” ì¤‘..."}
                    toxic_result, toxic_percent = predict_toxic_clause(toxic_model, sentence, 0.5011)
                else:
                    toxic_result, toxic_percent = 0, 0

                if unfair_result or toxic_result:
                    indentification_results.append({
                        'contract_article_number': article_number,
                        'contract_clause_number': clause_number,
                        'contract_subclause_number': subclause_number,
                        'Sentence': sentence,
                        'Unfair': unfair_result,
                        'Unfair_percent': unfair_percent,
                        'Toxic': toxic_result,
                        'Toxic_percent': toxic_percent,
                    })

    # 5ï¸âƒ£ ê³„ì•½ì„œ ìš”ì•½
    print('ğŸ“ ê³„ì•½ì„œë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘...')
    yield {"step": "ê³„ì•½ì„œë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘..."}  # âœ… yield ì‚¬ìš©

    for article_number, article_detail in articles.items():
        match = re.match(r"(ì œ\s?\d+ì¡°(?:ì˜\s?\d+)?\s?)\[(.*?)\]\s?(.+)", article_detail, re.DOTALL)
        if not match:
            continue

        article_title = match.group(2)
        summary = article_summary_AI_ver2(prompt, article_detail)

        summary_results.append({
            'article_number': article_number,
            'article_title': article_title,
            'summary': summary
        })

    if await send_status_safe("ë¶„ì„ ì™„ë£Œ!"):
        if websocket and websocket.client_state.name == "CONNECTED":
            await websocket.send_json({"done": True, "results": (indentification_results, summary_results)})
"""