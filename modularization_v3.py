################################################################################################
# í•„ìš” íŒ¨í‚¤ì§€ import
################################################################################################
import os
import subprocess, pickle, openai, torch, json, os, re, nltk, numpy as np, torch.nn as nn
from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.metrics.pairwise import cosine_similarity

from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast

prompt = (
    "ë‹¤ìŒì€ ê³„ì•½ì„œì˜ ì¡°í•­ì…ë‹ˆë‹¤. ì´ ì¡°í•­ì˜ ì£¼ìš” ë‚´ìš©ì„ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ê°„ëµíˆ ìš”ì•½í•˜ì„¸ìš”:\n"
    "1. ì´ ì¡°í•­ì´ ê·œì •í•˜ëŠ” ì£¼ìš” ëª©ì  ë˜ëŠ” ëŒ€ìƒ\n"
    "2. ê°‘ê³¼ ì„ì˜ ê¶Œë¦¬ì™€ ì˜ë¬´\n"
    "3. ì´í–‰í•´ì•¼ í•  ì ˆì°¨ì™€ ì¡°ê±´\n"
    "4. ìœ„ë°˜ ì‹œ ê²°ê³¼ ë˜ëŠ” ì¡°ì¹˜\n\n"
    "ìš”ì•½ì€ ê° ê¸°ì¤€ì— ë”°ë¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë©°, ì¤‘ë³µì„ í”¼í•˜ì„¸ìš”. "
    "ì¡° ì œëª©ê³¼ ê´€ë ¨ëœ í•µì‹¬ ì •ë³´ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.\n\n"
)

######################## open API KEY path(ë„ì»¤ ë°°í¬ ì‹œ ì‚­ì œ) ########################
#ì§„ì„
# open_API_KEY_path =
# ê³„ìŠ¹
# open_API_KEY_path =
# ëª…ì¬
open_API_KEY_path = 'D:/Key/openAI_key.txt'
# ìƒëŒ€ê²½ë¡œ 
#open_API_KEY_path = 'D:/Key/openAI_key.txt'

######################## hwp5txt path ########################
# ë°°í¬ì‹œ ê²½ë¡œë¡œ
hwp5txt_exe_path = "/usr/local/bin/hwp5txt"
# local ê²½ë¡œ 
#hwp5txt_exe_path = 'hwp5txt'
################################################################################################
# HwpíŒŒì¼ì—ì„œ Text ì¶”ì¶œ í›„ txt íŒŒì¼ë¡œ ë³€í™˜
################################################################################################
def hwp5txt_to_txt(hwp_path, output_dir=None):
    if not os.path.exists(hwp_path):
        raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hwp_path}")

    if output_dir is None:
        output_dir = os.path.dirname(hwp_path)

    base_name = os.path.splitext(os.path.basename(hwp_path))[0]
    txt_file_path = os.path.join(output_dir, f"{base_name}.txt")

    # hwp5txt ëª…ë ¹ì–´ ì‹¤í–‰
    command = f"hwp5txt \"{hwp_path}\" > \"{txt_file_path}\""
    subprocess.run(command, shell=True, check=True)

    print(f"í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: {txt_file_path}")
    return txt_file_path

################################################################################################
# HwpíŒŒì¼ì—ì„œ Text ì¶”ì¶œ
################################################################################################
def hwp5txt_to_string(hwp_path):
    hwp5txt = os.getenv("HWP5TXT_PATH")
    if not hwp5txt:
        raise EnvironmentError("HWP5TXT_PATH í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not os.path.exists(hwp_path):
        raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hwp_path}")
    command = f"{hwp5txt} \"{hwp_path}\""
    result = subprocess.run(
        command,
        shell=True,
        capture_output=True,
        text=True,
        encoding='utf-8',
        errors='ignore'
    )
    extracted_text = result.stdout
    return extracted_text

################################################################################################
# ì „ì²´ ê³„ì•½ì„œ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„, ì¡°ë¥¼ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ ver2
################################################################################################
def contract_to_articles_ver2(txt):
    pattern, key_pattern = r'(ì œ\d+ì¡°(?:ì˜\d+)? \[.+?\])', r'ì œ(\d+)ì¡°(?:ì˜(\d+))? \[.+?\]'
    matches = list(re.finditer(pattern, txt))
    contract_sections = {}
    for i, match in enumerate(matches):
        section_title = match.group(0)
        start_idx = match.end()
        end_idx = matches[i + 1].start() if i + 1 < len(matches) else len(txt)
        section_content = txt[start_idx:end_idx].strip()
        section_content = section_content.replace('â€œ', '').replace('â€', '').replace('\n', '').replace('<í‘œ>','')
        match_title = re.match(key_pattern, section_title)
        main_num = match_title.group(1)
        try:
            sub_num = match_title.group(2)
        except IndexError:
            sub_num = None
        key = f"{main_num}-{sub_num}" if sub_num else main_num
        contract_sections[key] = section_title + ' ' + section_content
    return contract_sections
################################################################################################
# ì¡°ë¥¼ ë°›ì•„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
################################################################################################
def split_once_by_clauses(content):
    pattern = r"(â‘ |â‘¡|â‘¢|â‘£|â‘¤|â‘¥|â‘¦|â‘§|â‘¨|â‘©)"
    matches = list(re.finditer(pattern, content))
    result = []
    for i, match in enumerate(matches):
        end = match.end()
        if i + 1 < len(matches):
            next_start = matches[i + 1].start()
            clause_content = content[end:next_start].strip()
        else:
            clause_content = content[end:].strip()
        result.append(match.group())
        result.append(clause_content)
    return result

def split_once_by_sub_clauses(content):
    pattern = r"(\d+\.)"
    matches = list(re.finditer(pattern, content))
    result = []
    for i, match in enumerate(matches):
        end = match.end()
        if i + 1 < len(matches):
            next_start = matches[i + 1].start()
            clause_content = content[end:next_start].strip()
        else:
            clause_content = content[end:].strip()
        result.append(match.group())
        result.append(clause_content)
    return result


def article_to_sentences(article_number,article_title, article_content):
    symtostr = {
        "â‘ ": "1", "â‘¡": "2", "â‘¢": "3", "â‘£": "4", "â‘¤": "5",
        "â‘¥": "6", "â‘¦": "7", "â‘§": "8", "â‘¨": "9", "â‘©": "10"
    }
    sentences = []
    if 'â‘ ' in article_content:
        clause_sections = split_once_by_clauses(article_content)
        for i in range(0, len(clause_sections), 2):
            clause_number = clause_sections[i]
            clause_content = clause_sections[i + 1]
            if '1.' in clause_content:
                sub_clause_sections = split_once_by_sub_clauses(clause_content)
                for j in range(0, len(sub_clause_sections), 2):
                    sub_clause_number = sub_clause_sections[j]
                    sub_clause_content = sub_clause_sections[j + 1]
                    sentences.append([article_number.strip(), article_title.strip(), '', symtostr[clause_number].strip(), clause_content.split('1.')[0].strip(), sub_clause_number[0].strip(), sub_clause_content.strip()])
            else:
                sentences.append([article_number.strip(), article_title.strip(), '', symtostr[clause_number].strip(), clause_content.split('â‘ ')[0].strip(), '', ''])
    elif '1.' in article_content:
        sub_clause_sections = split_once_by_sub_clauses(article_content)
        for j in range(0, len(sub_clause_sections), 2):
            sub_clause_number = sub_clause_sections[j]
            sub_clause_content = sub_clause_sections[j + 1]
            sentences.append([article_number.strip(), article_title.strip(), article_content.split('1.')[0].strip(), '', '', sub_clause_number.strip(),sub_clause_content.strip()])
    else:
        sentences.append([article_number.strip(),article_title.strip(),article_content.strip(),'','','',''])
    return sentences
################################################################################################
# ëª¨ë¸ ë¡œë“œ
################################################################################################
def load_trained_model_statice(model_class, model_file):
    model = model_class().to(device)
    state_dict = torch.load(model_file, map_location=device)
    if isinstance(state_dict, dict):
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        return model
    else:
        raise TypeError(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {model_file} (ì˜ëª»ëœ ë°ì´í„° íƒ€ì… {type(state_dict)})")

################################################################################################
# í† í¬ë‚˜ì´ì§• ëª¨ë¸ ë¡œë“œ & ì „ì²´ ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë“œ
################################################################################################
def initialize_models():
    global unfair_model, article_model, toxic_model, toxic_tokenizer, law_data, law_embeddings, device, tokenizer, bert_model, summary_model_ver2, summary_tokenizer_ver2

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = BertTokenizer.from_pretrained("klue/bert-base")
    bert_model = BertModel.from_pretrained("klue/bert-base").to(device)
    nltk.data.path.append(f'./nltk_data')

    summary_model_ver2 = BartForConditionalGeneration.from_pretrained('./model/article_summary_ver2/')
    summary_tokenizer_ver2 = PreTrainedTokenizerFast.from_pretrained('./model/article_summary_ver2/')

    class BertMLPClassifier(nn.Module):
        def __init__(self, bert_model_name="klue/bert-base", hidden_size=256):
            super(BertMLPClassifier, self).__init__()
            self.bert = BertModel.from_pretrained(bert_model_name)
            self.fc1 = nn.Linear(self.bert.config.hidden_size, hidden_size)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.3)
            self.fc2 = nn.Linear(hidden_size, 1)  # ë¶ˆê³µì •(1) í™•ë¥ ì„ ì¶œë ¥
            self.sigmoid = nn.Sigmoid()  # í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜
        def forward(self, input_ids, attention_mask):
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] ë²¡í„° ì‚¬ìš©
            x = self.fc1(cls_output)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.fc2(x)
            return self.sigmoid(x)  # 0~1 í™•ë¥ ê°’ ë°˜í™˜
    class BertArticleClassifier(nn.Module):
        def __init__(self, bert_model_name="klue/bert-base", hidden_size=256, num_classes=27):
            super(BertArticleClassifier, self).__init__()
            self.bert = BertModel.from_pretrained(bert_model_name)
            self.fc1 = nn.Linear(self.bert.config.hidden_size, hidden_size)
            self.relu = nn.ReLU()
            self.dropout = nn.Dropout(0.3)
            self.fc2 = nn.Linear(hidden_size, num_classes)  # ì¡°í•­ ê°œìˆ˜ë§Œí¼ ì¶œë ¥
            self.softmax = nn.Softmax(dim=1)

        def forward(self, input_ids, attention_mask):
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] ë²¡í„° ì‚¬ìš©
            x = self.fc1(cls_output)
            x = self.relu(x)
            x = self.dropout(x)
            x = self.fc2(x)
            return self.softmax(x)  # í™•ë¥  ë¶„í¬ ì¶œë ¥
    # ë¶ˆê³µì • ì¡°í•­ íŒë³„ ëª¨ë¸ ë¡œë“œ
    unfair_model = load_trained_model_statice(BertMLPClassifier, f"./model/unfair_identification/klue_bert_mlp.pth")

    # ì¡°í•­ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë“œ
    article_model = load_trained_model_statice(BertArticleClassifier, f"./model//article_prediction/klue_bert_mlp.pth")

    # ë…ì†Œ ì¡°í•­ íŒë³„ ëª¨ë¸ ë¡œë“œ
    toxic_model = load_trained_model_statice(BertMLPClassifier, f"./model/toxic_identification/klue_bert_mlp.pth")

    # ë²•ë¥  ë°ì´í„° ë¡œë“œ
    # with open("./Data/law_embeddings.pkl", "rb") as f:
    with open("./Data/law_embeddings.pkl", "rb") as f:
        data = pickle.load(f)
    # with open("./Data/law_data_ver2.json", "r", encoding="utf-8") as f:
    with open("./Data/law_data_ver2.json", "r", encoding="utf-8") as f:
        law_data = json.load(f)
    law_embeddings = np.array(data["law_embeddings"])

    print("âœ… ëª¨ë“  ëª¨ë¸ ë° ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
################################################################################################
# ë¶ˆê³µì • ì‹ë³„
################################################################################################
def predict_unfair_clause(model, sentence, threshold=0.5):
    model.eval()
    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=256, return_tensors="pt").to(device)
    with torch.no_grad():
        output = model(inputs["input_ids"], inputs["attention_mask"])
        unfair_prob = output.item()
    return 1 if unfair_prob >= threshold else 0, unfair_prob
################################################################################################
# ì¡°í•­ ì˜ˆì¸¡
################################################################################################
def predict_article(model,sentence):
    model.eval()
    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=256, return_tensors="pt").to(device)
    with torch.no_grad():
        output = model(inputs["input_ids"], inputs["attention_mask"])
        predicted_idx = torch.argmax(output).item()
        predicted_article = predicted_idx + 4
    return predicted_article
################################################################################################
# ë…ì†Œ ì‹ë³„
################################################################################################
def predict_toxic_clause(c_model, sentence, threshold=0.5):
    c_model.eval()
    inputs = tokenizer(sentence, padding=True, truncation=True, max_length=256, return_tensors="pt").to(device)
    with torch.no_grad():
        output = c_model(inputs["input_ids"], inputs["attention_mask"])
        toxic_prob = output.item()
    return 1 if toxic_prob >= threshold else 0, toxic_prob
################################################################################################
# ì½”ì‚¬ì¸ ìœ ì‚¬ë„
################################################################################################
def find_most_similar_law_within_article(sentence, predicted_article, law_data):
    contract_embedding = bert_model(**tokenizer(sentence, return_tensors="pt", padding=True, truncation=True).to(device)).pooler_output.cpu().detach().numpy()[0]
    predicted_article = str(predicted_article)
    matching_article = []
    for article in law_data:
        if article["article_number"].split()[1].startswith(predicted_article):
            matching_article.append(article)
    if not matching_article:
        return {
            "Article number": None,
            "Article title": None,
            "Paragraph number": None,
            "Subparagraph number": None,
            "Article detail": None,
            "Paragraph detail": None,
            "Subparagraph detail": None,
            "similarity": None
        }
    best_match = None
    best_similarity = -1
    for article in matching_article:
        article_title = article.get("article_title", None)
        article_detail = article.get("article_content", None)
        if article_title:
            article_embedding = bert_model(**tokenizer(article_title, return_tensors="pt", padding=True, truncation=True).to(device)).pooler_output.cpu().detach().numpy()[0]
            similarity = cosine_similarity([contract_embedding], [article_embedding])[0][0]
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = {
                    "article_number": article["article_number"],
                    "article_title": article_title,
                    "article_detail": article_detail,
                    "paragraph_number": None,
                    "paragraph_detail": None,
                    "subparagraphs": None,
                    "similarity": best_similarity
                }
        for clause in article.get("clauses", []):
            clause_text = clause["content"].strip()
            clause_number = clause["clause_number"]
            if clause_text:
                clause_embedding = bert_model(**tokenizer(clause_text, return_tensors="pt", padding=True, truncation=True).to(device)).pooler_output.cpu().detach().numpy()[0]
                similarity = cosine_similarity([contract_embedding], [clause_embedding])[0][0]
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = {
                        "article_number": article["article_number"],
                        "article_title": article_title,
                        "article_detail": article_detail,
                        "paragraph_number": clause_number,
                        "paragraph_detail": clause_text,
                        "subparagraphs": clause.get("sub_clauses", []),
                        "similarity": best_similarity
                    }
        if best_match and best_match["subparagraphs"]:
            for subclause in best_match["subparagraphs"]:
                if isinstance(subclause, str):
                    subclause_embedding = bert_model(**tokenizer(subclause, return_tensors="pt", padding=True, truncation=True).to(device)).pooler_output.cpu().detach().numpy()[0]
                    similarity = cosine_similarity([contract_embedding], [subclause_embedding])[0][0]
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_match["subparagraph_detail"] = subclause
    if best_match is None:
        return {
            "Article number": f"Article {predicted_article}",
            "Article title": None,
            "Paragraph number": None,
            "Subparagraph number": None,
            "Article detail": None,
            "Paragraph detail": None,
            "Subparagraph detail": None,
            "similarity": None
        }
    return {
        "Article number": best_match["article_number"],
        "Article title": best_match["article_title"],
        "Paragraph number": f"Paragraph {best_match['paragraph_number']}" if best_match["paragraph_number"] else None,
        "Subparagraph number": "Subparagraph" if best_match.get("subparagraph_detail") else None,
        "Article detail": best_match["article_detail"],
        "Paragraph detail": best_match["paragraph_detail"],
        "Subparagraph detail": best_match.get("subparagraph_detail", None),
        "similarity": best_similarity
    }
################################################################################################
# ì„¤ëª… AI
################################################################################################
def explanation_AI(sentence, unfair_label, toxic_label, law=None):
    with open(open_API_KEY_path, 'r') as file:
        openai.api_key = file.readline().strip()
    os.environ['OPENAI_API_KEY'] = openai.api_key
    client = openai.OpenAI()
    if unfair_label == 0 and toxic_label == 0:
        return None
    prompt = f"""
        ì•„ë˜ ê³„ì•½ ì¡°í•­ì´ íŠ¹ì • ë²•ë¥ ì„ ìœ„ë°˜í•˜ëŠ”ì§€ ë¶„ì„í•˜ê³ , ì¡°í•­(ì œnì¡°), í•­(ì œmí•­), í˜¸(ì œzí˜¸) í˜•ì‹ìœ¼ë¡œ **ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ** ì„¤ëª…í•˜ì„¸ìš”.
        ğŸ“Œ **ì„¤ëª…í•  ë•ŒëŠ” ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ë§í•˜ëŠ” ë“¯í•œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.**
        ğŸ“Œ **í•œëˆˆì— ë³´ê¸° ì‰½ë„ë¡ ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”.**
        ğŸ“Œ **ë¶ˆê³µì • ë¼ë²¨ì´ 1ì¸ ê²½ìš°ì—ëŠ” ë¶ˆê³µì •ì— ê´€í•œ ì„¤ëª…ë§Œ í•˜ê³ , ë…ì†Œ ë¼ë²¨ì´ 1ì¸ ê²½ìš°ì—ëŠ” ë…ì†Œì— ê´€í•œ ì„¤ëª…í•œ í•˜ì„¸ìš”**

        ê³„ì•½ ì¡°í•­: "{sentence}"
        ë¶ˆê³µì • ë¼ë²¨: {unfair_label} (1ì¼ ê²½ìš° ë¶ˆê³µì •)
        ë…ì†Œ ë¼ë²¨: {toxic_label} (1ì¼ ê²½ìš° ë…ì†Œ)   
        {f"ê´€ë ¨ ë²• ì¡°í•­: {law}" if law else "ê´€ë ¨ ë²• ì¡°í•­ ì—†ìŒ"}

        ğŸ”´ **ë¶ˆê³µì • ì¡°í•­ì¼ ê²½ìš°:**
        1ï¸âƒ£ **ìœ„ë°˜ëœ ë²• ì¡°í•­ì„ 'ì œnì¡° ì œmí•­ ì œzí˜¸' í˜•ì‹ìœ¼ë¡œ ë¨¼ì € ë§í•´ì£¼ì„¸ìš”.**
        2ï¸âƒ£ **ìœ„ë°˜ ì´ìœ ë¥¼ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.**
        3ï¸âƒ£ **ì„¤ëª…ì€ 'ğŸš¨ ë²• ìœ„ë°˜!', 'ğŸ” ì´ìœ ' ìˆœì„œë¡œ êµ¬ì„±í•˜ì„¸ìš”.**

        âš« **ë…ì†Œ ì¡°í•­ì¼ ê²½ìš°:**
        1ï¸âƒ£ **ë²• ìœ„ë°˜ì´ ì•„ë‹ˆë¼ë©´, í•´ë‹¹ ì¡°í•­ì´ ê³„ì•½ ë‹¹ì‚¬ìì—ê²Œ ì–´ë–¤ ìœ„í—˜ì„ ì´ˆë˜í•˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.**
        2ï¸âƒ£ **êµ¬ì²´ì ì¸ ë¬¸ì œì ì„ ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.**
        3ï¸âƒ£ **ì„¤ëª…ì€ 'ğŸ’€ ë…ì†Œ ì¡°í•­', 'ğŸ” ì´ìœ ' ìˆœì„œë¡œ êµ¬ì„±í•˜ì„¸ìš”.**

        âš ï¸ ì°¸ê³ : ì œê³µëœ ë²• ì¡°í•­ì´ ì‹¤ì œë¡œ ìœ„ë°˜ëœ ì¡°í•­ì´ ì•„ë‹ ê²½ìš°, **GPTê°€ íŒë‹¨í•œ ì ì ˆí•œ ë²• ì¡°í•­ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.** 
        ê·¸ëŸ¬ë‚˜ ì›ë˜ ì œê³µëœ ë²• ì¡°í•­ê³¼ ë¹„êµí•˜ì—¬ ë°˜ë°•í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.
    """
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "system", "content":
                            "ë‹¹ì‹ ì€ ê³„ì•½ì„œ ì¡°í•­ì´ íŠ¹ì • ë²•ë¥ ì„ ìœ„ë°˜í•˜ëŠ”ì§€ ë¶„ì„í•˜ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. \
                            ë¶ˆê³µì • ì¡°í•­ì˜ ê²½ìš°, ì–´ë–¤ ë²• ì¡°í•­ì„ ìœ„ë°˜í–ˆëŠ”ì§€ ì¡°í•­(ì œnì¡°), í•­(ì œmí•­), í˜¸(ì œzí˜¸) í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ëª…ì‹œí•œ í›„ ì„¤ëª…í•˜ì„¸ìš”. \
                            ë§Œì•½ ì œê³µëœ ë²• ì¡°í•­ì´ ì‹¤ì œë¡œ ìœ„ë°˜ëœ ì¡°í•­ì´ ì•„ë‹ˆë¼ë©´, GPTê°€ íŒë‹¨í•œ ì ì ˆí•œ ë²• ì¡°í•­ì„ ì‚¬ìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”. \
                            ë…ì†Œ ì¡°í•­ì€ ë²•ë¥  ìœ„ë°˜ì´ ì•„ë‹ˆë¼ ê³„ì•½ ë‹¹ì‚¬ìì—ê²Œ ë¯¸ì¹˜ëŠ” ìœ„í—˜ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”."
                   },
                  {"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=300
    ).choices[0].message.content
    return response

################################################################################################
# ìš”ì•½ AI
################################################################################################
def article_summary_AI_ver2(prompt, article, max_length=256):
    input_ids = summary_tokenizer_ver2(f"{prompt}{article}", return_tensors="pt").input_ids
    summary_ids = summary_model_ver2.generate(input_ids, max_length=max_length, num_beams=4, early_stopping=True)
    summary = summary_tokenizer_ver2.decode(summary_ids[0], skip_special_tokens=True)
    return summary
################################################################################################
# íŒŒì´í”„ ë¼ì¸
################################################################################################
def pipline(contract_path):
    indentification_results = []
    summary_results = []
    print('í•œê¸€ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ')
    txt = hwp5txt_to_string(contract_path)
    print('í…ìŠ¤íŠ¸ë¥¼ ì¡° ë‹¨ìœ„ë¡œ ë¶„ë¦¬')
    articles = contract_to_articles_ver2(txt)
    for article_number, article_detail in articles.items():
        print(f'*******************{article_number}ì¡° ë¬¸ì¥ ë¶„ë¦¬ ì‹œì‘*******************')
        match = re.match(r"(ì œ\s?\d+ì¡°(?:ì˜\s?\d+)?\s?)\[(.*?)\]\s?(.+)", article_detail, re.DOTALL)
        article_title = match.group(2)
        article_content = match.group(3)
        sentences = article_to_sentences(article_number,article_title, article_content)
        summary = article_summary_AI_ver2(prompt, article_detail)
        summary_results.append(
                        {
                        'article_number':article_number, # ì¡° ë²ˆí˜¸
                        'article_title': article_title, # ì¡° ì œëª©
                        'summary': summary # ì¡° ìš”ì•½
                        }
        )
        print(f'{article_number}ì¡° ìš”ì•½: {summary}')
        pre_article_detail = ''
        pre_clause_detail = ''
        pre_subclause_detail= ''
        for _, _, _, clause_number, clause_detail, subclause_number, subclause_detail in sentences:
            for idx,sentence in enumerate([article_detail, clause_detail, subclause_detail]):
                if idx==0:
                    if pre_article_detail == sentence:
                        continue
                    else:
                        pre_article_detail = sentence
                elif idx==1:
                    if pre_clause_detail == sentence:
                        continue
                    else:
                        pre_clause_detail = sentence

                elif idx==2:
                    if pre_subclause_detail == sentence:
                        continue
                    else:
                        pre_subclause_detail = sentence


                # print(f'sentence: {sentence}')
                unfair_result, unfair_percent = predict_unfair_clause(unfair_model, sentence, 0.5011)
                if unfair_result:
                    # print('ë¶ˆê³µì •!!!')
                    predicted_article = predict_article(article_model, sentence)  # ì˜ˆì¸¡ëœ ì¡°í•­
                    law_details = find_most_similar_law_within_article(sentence, predicted_article, law_data)
                    toxic_result = 0
                    toxic_percent = 0
                else:
                    toxic_result, toxic_percent = predict_toxic_clause(toxic_model, sentence, 0.5011)
                    # print('ë…ì†Œ!!!' if toxic_result else 'ì¼ë°˜!!!')
                    law_details = {
                        "Article number": None,
                        "Article title": None,
                        "Paragraph number": None,
                        "Subparagraph number": None,
                        "Article detail": None,
                        "Paragraph detail": None,
                        "Subparagraph detail": None,
                        "similarity": None
                    }
                law_text = []
                if law_details.get("Article number"):
                    law_text.append(f"{law_details['Article number']}({law_details['Article title']})")
                if law_details.get("Article detail"):
                    law_text.append(f": {law_details['Article detail']}")
                if law_details.get("Paragraph number"):
                    law_text.append(f" {law_details['Paragraph number']}: {law_details['Paragraph detail']}")
                if law_details.get("Subparagraph number"):
                    law_text.append(f" {law_details['Subparagraph number']}: {law_details['Subparagraph detail']}")
                law = "".join(law_text) if law_text else None

                # explain = explanation_AI(sentence, unfair_result, toxic_result, law)

                if unfair_result or toxic_result:
                    indentification_results.append(
                                    {
                                        'contract_article_number': article_number, # ê³„ì•½ì„œ ì¡°
                                        'contract_clause_number' : clause_number, # ê³„ì•½ì„œ í•­
                                        'contract_subclause_number': subclause_number, # ê³„ì•½ì„œ í˜¸
                                        'Sentence': sentence, # ì‹ë³„
                                        'Unfair': unfair_result, # ë¶ˆê³µì • ì—¬ë¶€
                                        'Unfair_percent': unfair_percent, # ë¶ˆê³µì • í™•ë¥ 
                                        'Toxic': toxic_result,  # ë…ì†Œ ì—¬ë¶€
                                        'Toxic_percent': toxic_percent,  # ë…ì†Œ í™•ë¥ 
                                        'law_article_number': law_details['Article number'],  # ì–´ê¸´ ë²• ì¡°   (ë¶ˆê³µì • 1ì¼ë•Œ, ì•„ë‹ˆë©´ None)
                                        'law_clause_number_law': law_details['Paragraph number'], # ì–´ê¸´ ë²• í•­ (ë¶ˆê³µì • 1ì¼ë•Œ, ì•„ë‹ˆë©´ None)
                                        'law_subclause_numbe_lawr': law_details['Subparagraph number'],  # ì–´ê¸´ ë²• í˜¸ (ë¶ˆê³µì • 1ì¼ë•Œ, ì•„ë‹ˆë©´ None)
                                        'explain': None #explain (ë¶ˆê³µì • 1ë˜ëŠ” ë…ì†Œ 1ì¼ë•Œ, ì•„ë‹ˆë©´ None)
                                        }
                    )
    return indentification_results, summary_results
    ############################################################################################################
    ############################################################################################################
